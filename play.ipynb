{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mcw\\Envs\\gpt-scratch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, but what I'm really doing is making a human-readable document. There are other languages, but those are\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not a syntax model. That's why I like it. I've done a lot of programming projects.\\n\"},\n",
       " {'generated_text': \"Hello, I'm a language model, and I'll do it in no time!\\n\\nOne of the things we learned from talking to my friend\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not a command line tool.\\n\\nIf my code is simple enough:\\n\\nif (use (string\"},\n",
       " {'generated_text': \"Hello, I'm a language model, I've been using Language in all my work. Just a small example, let's see a simplified example.\"}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The stars and galaxies are visible because our light was emitted so fast that we only see them when we'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"The stars and galaxies are\", max_length=20, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight torch.Size([50257, 768])\n",
      "transformer.wpe.weight torch.Size([1024, 768])\n",
      "transformer.h.0.ln_1.weight torch.Size([768])\n",
      "transformer.h.0.ln_1.bias torch.Size([768])\n",
      "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.0.ln_2.weight torch.Size([768])\n",
      "transformer.h.0.ln_2.bias torch.Size([768])\n",
      "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_1.weight torch.Size([768])\n",
      "transformer.h.1.ln_1.bias torch.Size([768])\n",
      "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_2.weight torch.Size([768])\n",
      "transformer.h.1.ln_2.bias torch.Size([768])\n",
      "transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_1.weight torch.Size([768])\n",
      "transformer.h.2.ln_1.bias torch.Size([768])\n",
      "transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.2.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.2.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.2.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_2.weight torch.Size([768])\n",
      "transformer.h.2.ln_2.bias torch.Size([768])\n",
      "transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.2.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.2.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_1.weight torch.Size([768])\n",
      "transformer.h.3.ln_1.bias torch.Size([768])\n",
      "transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.3.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.3.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.3.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_2.weight torch.Size([768])\n",
      "transformer.h.3.ln_2.bias torch.Size([768])\n",
      "transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.3.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.3.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_1.weight torch.Size([768])\n",
      "transformer.h.4.ln_1.bias torch.Size([768])\n",
      "transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.4.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.4.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.4.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_2.weight torch.Size([768])\n",
      "transformer.h.4.ln_2.bias torch.Size([768])\n",
      "transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.4.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.4.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_1.weight torch.Size([768])\n",
      "transformer.h.5.ln_1.bias torch.Size([768])\n",
      "transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.5.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.5.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.5.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_2.weight torch.Size([768])\n",
      "transformer.h.5.ln_2.bias torch.Size([768])\n",
      "transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.5.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_1.weight torch.Size([768])\n",
      "transformer.h.6.ln_1.bias torch.Size([768])\n",
      "transformer.h.6.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.6.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.6.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.6.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_2.weight torch.Size([768])\n",
      "transformer.h.6.ln_2.bias torch.Size([768])\n",
      "transformer.h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.6.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.6.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_1.weight torch.Size([768])\n",
      "transformer.h.7.ln_1.bias torch.Size([768])\n",
      "transformer.h.7.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.7.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.7.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.7.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_2.weight torch.Size([768])\n",
      "transformer.h.7.ln_2.bias torch.Size([768])\n",
      "transformer.h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.7.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.7.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_1.weight torch.Size([768])\n",
      "transformer.h.8.ln_1.bias torch.Size([768])\n",
      "transformer.h.8.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.8.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.8.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.8.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_2.weight torch.Size([768])\n",
      "transformer.h.8.ln_2.bias torch.Size([768])\n",
      "transformer.h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.8.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.8.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_1.weight torch.Size([768])\n",
      "transformer.h.9.ln_1.bias torch.Size([768])\n",
      "transformer.h.9.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.9.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.9.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.9.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_2.weight torch.Size([768])\n",
      "transformer.h.9.ln_2.bias torch.Size([768])\n",
      "transformer.h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.9.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.9.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_1.weight torch.Size([768])\n",
      "transformer.h.10.ln_1.bias torch.Size([768])\n",
      "transformer.h.10.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.10.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.10.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.10.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_2.weight torch.Size([768])\n",
      "transformer.h.10.ln_2.bias torch.Size([768])\n",
      "transformer.h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.10.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.10.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_1.weight torch.Size([768])\n",
      "transformer.h.11.ln_1.bias torch.Size([768])\n",
      "transformer.h.11.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.11.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.11.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.11.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_2.weight torch.Size([768])\n",
      "transformer.h.11.ln_2.bias torch.Size([768])\n",
      "transformer.h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.11.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.ln_f.weight torch.Size([768])\n",
      "transformer.ln_f.bias torch.Size([768])\n",
      "lm_head.weight torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\") # 124M parameters\n",
    "\n",
    "for k, v in model.state_dict().items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Token Embeddings**\n",
    "\n",
    "Token embeddings wte.weights torch.Size([50257, 768]) are basically a lookup table that contains the embeddings of each token in the vocabulary. The vocabulary size is 50257, and the embedding size is 768.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50257, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['transformer.wte.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 50257\n",
      "\n",
      "First 10 tokens:\n",
      "0: '!'\n",
      "1: '\"'\n",
      "2: '#'\n",
      "3: '$'\n",
      "4: '%'\n",
      "5: '&'\n",
      "6: \"'\"\n",
      "7: '('\n",
      "8: ')'\n",
      "9: '*'\n",
      "\n",
      "Special tokens:\n",
      "pad_token: None\n",
      "eos_token: '<|endoftext|>'\n",
      "unk_token: '<|endoftext|>'\n",
      "bos_token: '<|endoftext|>'\n",
      "\n",
      "Sample encodings:\n",
      "Hello: [15496]\n",
      "World: [10603]\n",
      "Hello World: [15496, 2159]\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "def load_and_analyze_vocab():\n",
    "    # Load the GPT-2 tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    \n",
    "    # Get the vocabulary as a dictionary\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    \n",
    "    # Sort vocabulary by token index\n",
    "    sorted_vocab = OrderedDict(sorted(vocab.items(), key=lambda x: x[1]))\n",
    "    \n",
    "    # Create a more detailed vocabulary analysis\n",
    "    vocab_analysis = {\n",
    "        'token_to_id': sorted_vocab,\n",
    "        'id_to_token': {v: k for k, v in sorted_vocab.items()},\n",
    "        'special_tokens': {\n",
    "            'pad_token': tokenizer.pad_token,\n",
    "            'eos_token': tokenizer.eos_token,\n",
    "            'unk_token': tokenizer.unk_token,\n",
    "            'bos_token': tokenizer.bos_token\n",
    "        },\n",
    "        'vocab_size': len(vocab),\n",
    "        'sample_encodings': {\n",
    "            'Hello': tokenizer.encode('Hello'),\n",
    "            'World': tokenizer.encode('World'),\n",
    "            'Hello World': tokenizer.encode('Hello World')\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return vocab_analysis\n",
    "\n",
    "# Load and analyze vocabulary\n",
    "vocab_info = load_and_analyze_vocab()\n",
    "\n",
    "# Save vocabulary to JSON file\n",
    "with open('gpt2_vocabulary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(vocab_info, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Print some basic statistics and examples\n",
    "print(f\"Vocabulary size: {vocab_info['vocab_size']}\")\n",
    "print(\"\\nFirst 10 tokens:\")\n",
    "for i, (token, idx) in enumerate(list(vocab_info['token_to_id'].items())[:10]):\n",
    "    print(f\"{idx}: {repr(token)}\")\n",
    "\n",
    "print(\"\\nSpecial tokens:\")\n",
    "for token_type, token in vocab_info['special_tokens'].items():\n",
    "    print(f\"{token_type}: {repr(token)}\")\n",
    "\n",
    "print(\"\\nSample encodings:\")\n",
    "for text, encoding in vocab_info['sample_encodings'].items():\n",
    "    print(f\"{text}: {encoding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tokenize using byte-pair encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Deep learning is one of the revolutionary technologies in the 21st century!\n",
      "Tokens: [29744, 4673, 318, 530, 286, 262, 12253, 8514, 287, 262, 2310, 301, 4289, 0]\n",
      "Decoded text: Deep learning is one of the revolutionary technologies in the 21st century!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import regex as re\n",
    "from typing import List\n",
    "\n",
    "class SimpleGPT2Tokenizer:\n",
    "    def __init__(self):\n",
    "        # Load GPT-2 encoder, decoder and byte_encoder\n",
    "        with open('utils/vocab/gpt2_vocabulary.json', 'r', encoding='utf-8') as f:\n",
    "            self.encoder = json.load(f)['token_to_id']\n",
    "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
    "        \n",
    "        # Initialize byte-level encodings\n",
    "        self._initialize_byte_encodings()\n",
    "        \n",
    "        # GPT-2 regex pattern\n",
    "        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "    \n",
    "    def _initialize_byte_encodings(self):\n",
    "        bytes_to_unicode_list = list(range(ord('!'), ord('~')+1)) + \\\n",
    "                              list(range(ord('¡'), ord('¬')+1)) + \\\n",
    "                              list(range(ord('®'), ord('ÿ')+1))\n",
    "        cs = bytes_to_unicode_list.copy()\n",
    "        n = 0\n",
    "        \n",
    "        # Add missing bytes\n",
    "        for b in range(256):\n",
    "            if b not in bytes_to_unicode_list:\n",
    "                bytes_to_unicode_list.append(b)\n",
    "                cs.append(256 + n)\n",
    "                n += 1\n",
    "                \n",
    "        cs = [chr(n) for n in cs]\n",
    "        self.byte_encoder = dict(zip(bytes_to_unicode_list, cs))\n",
    "        self.byte_decoder = {v:k for k,v in self.byte_encoder.items()}\n",
    "\n",
    "    def byte_encode(self, text: str) -> str:\n",
    "        \"\"\"Convert text to bytes then to unicode representation\"\"\"\n",
    "        return ''.join([self.byte_encoder[b] for b in text.encode('utf-8')])\n",
    "\n",
    "    def byte_decode(self, text: str) -> str:\n",
    "        \"\"\"Convert unicode representation back to text\"\"\"\n",
    "        return bytes(self.byte_decoder[c] for c in text).decode('utf-8', errors='replace')\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"Convert text to tokens using greedy longest match algorithm\"\"\"\n",
    "        bpe_tokens = []\n",
    "        byte_encoded = self.byte_encode(text)\n",
    "        i = 0\n",
    "        \n",
    "        # Longest match algorithm - sliding window approach from left to right\n",
    "        while i < len(byte_encoded):\n",
    "            longest_match = None\n",
    "            longest_length = 0\n",
    "            \n",
    "            # Try to find the longest matching token starting at position i\n",
    "            for j in range(i + 1, len(byte_encoded) + 1):\n",
    "                current_substr = byte_encoded[i:j]\n",
    "                if current_substr in self.encoder:\n",
    "                    if len(current_substr) > longest_length:\n",
    "                        longest_match = current_substr\n",
    "                        longest_length = len(current_substr)\n",
    "            \n",
    "            if longest_match:\n",
    "                # Add the longest matching token\n",
    "                bpe_tokens.append(self.encoder[longest_match])\n",
    "                i += longest_length\n",
    "            else:\n",
    "                # Handle unknown tokens character by character\n",
    "                char = byte_encoded[i]\n",
    "                if char in self.encoder:\n",
    "                    bpe_tokens.append(self.encoder[char])\n",
    "                i += 1\n",
    "        \n",
    "        return bpe_tokens\n",
    "\n",
    "    def decode(self, tokens: List[int]) -> str:\n",
    "        \"\"\"Convert tokens back to text\"\"\"\n",
    "        text = ''.join(self.decoder[token] for token in tokens)\n",
    "        return self.byte_decode(text)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "tokenizer = SimpleGPT2Tokenizer()\n",
    "text = \"Deep learning is one of the revolutionary technologies in the 21st century!\"\n",
    "\n",
    "# Encode and decode\n",
    "tokens = tokenizer.encode(text)\n",
    "decoded_text = tokenizer.decode(tokens)\n",
    "\n",
    "print(f\"Original text: {text}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Decoded text: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the corresponding embeddings for each token from the lookup table\n",
    "token_embeddings = model.state_dict()['transformer.wte.weight'][tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Include the batch dimension shape (1, num_tokens, embedding_dim)\n",
    "token_embeddings = token_embeddings.unsqueeze(0)\n",
    "\n",
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# Get the positional embeddings for each token\n",
    "\n",
    "position_ids = torch.arange(len(tokens)).unsqueeze(0)\n",
    "\n",
    "position_embeddings = model.state_dict()['transformer.wpe.weight'][position_ids]\n",
    "position_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate token embeddings and positional embeddings\n",
    "\n",
    "embeddings = token_embeddings + position_embeddings\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0830, -0.2365,  0.1785,  ..., -0.1328,  0.0512,  0.1872],\n",
       "         [ 0.0478, -0.0467, -0.0417,  ..., -0.0668,  0.0224, -0.0033],\n",
       "         [-0.0055, -0.0747,  0.1101,  ...,  0.1342, -0.0187, -0.0468],\n",
       "         ...,\n",
       "         [ 0.0711, -0.0996,  0.0806,  ..., -0.0234, -0.1299, -0.1057],\n",
       "         [-0.0567,  0.0671,  0.2829,  ..., -0.0525, -0.0687, -0.2013],\n",
       "         [-0.1060, -0.0190,  0.1377,  ..., -0.1406,  0.0136,  0.0454]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Layer Normalization in Transformers**\n",
    "\n",
    "Layer Normalization is a technique used to normalize the activations in neural network layers, particularly in transformers.\n",
    "\n",
    "\n",
    "Given an input vector $$ x = (x_1, x_2, \\dots, x_H) $$ for a particular sample with $$ (H) $$ features:\n",
    "\n",
    "1. **Compute Mean:**  \n",
    "   $$ \\mu = \\frac{1}{H} \\sum_{i=1}^{H} x_i $$\n",
    "\n",
    "2. **Compute Variance:**  \n",
    "   $$ \\sigma^2 = \\frac{1}{H} \\sum_{i=1}^{H} (x_i - \\mu)^2 $$\n",
    "\n",
    "3. **Normalize Each Feature:**  \n",
    "   $$ \\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} $$  \n",
    "   where \\( \\epsilon \\) is a small constant for numerical stability.\n",
    "\n",
    "4. **Apply Scale and Shift:**  \n",
    "   $$ y_i = \\gamma \\hat{x}_i + \\beta $$  \n",
    "   where \\( \\gamma \\) (scale) and \\( \\beta \\) (shift) are learnable parameters.\n",
    "\n",
    "---\n",
    "\n",
    "### **LayerNorm in Transformer Architecture**\n",
    "\n",
    "In a Transformer block, LayerNorm is typically applied as:\n",
    "\n",
    "$$ \\text{Output} = \\text{LayerNorm}(x + \\text{SubLayer}(x)) $$\n",
    "\n",
    "Where:\n",
    "- \\( x \\) is the input to the sub-layer.\n",
    "- \"SubLayer\" could be the self-attention mechanism or a feed-forward layer.\n",
    "- LayerNorm ensures stable gradient flow and better performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n",
      "torch.Size([768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.9977e-02, -8.4150e-02,  9.8113e-03,  ..., -7.5416e-02,\n",
       "           1.2459e-02,  8.0012e-02],\n",
       "         [ 4.6974e-02, -9.2930e-03, -9.1281e-02,  ..., -6.6427e-02,\n",
       "           6.2249e-03, -1.6448e-02],\n",
       "         [-5.2359e-03, -4.0788e-02,  2.8857e-02,  ...,  1.2823e-01,\n",
       "          -2.8203e-02, -5.8558e-02],\n",
       "         ...,\n",
       "         [ 8.2508e-02, -6.6363e-02,  2.8833e-03,  ..., -3.2398e-02,\n",
       "          -1.3710e-01, -1.1933e-01],\n",
       "         [-7.0218e-02,  9.3280e-02,  1.6860e-01,  ..., -6.3457e-02,\n",
       "          -8.0316e-02, -2.1896e-01],\n",
       "         [-1.3806e-01,  7.8144e-03,  5.6480e-02,  ..., -1.6285e-01,\n",
       "           2.1035e-04,  3.3637e-02]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln1_weight = model.state_dict()['transformer.h.0.ln_1.weight']\n",
    "ln1_bias = model.state_dict()['transformer.h.0.ln_1.bias']\n",
    "\n",
    "print(ln1_weight.shape)\n",
    "print(ln1_bias.shape)\n",
    "# Define layer normalization function\n",
    "def layer_norm(x, weight, bias, eps=1e-5):\n",
    "    mean = x.mean(-1, keepdim=True)\n",
    "    std = x.std(-1, keepdim=True)\n",
    "    norm_x = (x - mean) / (std + eps)\n",
    "\n",
    "    # Scaling and shifiing using weight and bias parameters\n",
    "    return norm_x * weight + bias\n",
    "\n",
    "# Apply layer normalization\n",
    "ln1_output = layer_norm(embeddings, ln1_weight, ln1_bias)\n",
    "ln1_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Self Attention Mechanism in Transformers**\n",
    "\n",
    "Self-attention mechanism computes a weighted sum of all values based on the similarity between queries and keys. The main steps are:\n",
    "\n",
    "1. **Input Projections**  \n",
    "    Query, Key, Value matrices are obtained by projecting input embeddings:\n",
    "    $$ Q = XW_Q, K = XW_K, V = XW_V $$\n",
    "    where $X \\in \\mathbb{R}^{L \\times d_{model}}$ is the input and $W_Q, W_K, W_V \\in \\mathbb{R}^{d_{model} \\times d_k}$ are learnable parameters.\n",
    "\n",
    "2. **Multi-head Splitting**  \n",
    "    Split Q, K, V into h heads:\n",
    "    $$ Q_i, K_i, V_i \\in \\mathbb{R}^{L \\times (d_k/h)} $$\n",
    "    where $i \\in [1,h]$ represents each attention head.\n",
    "\n",
    "3. **Scaled Dot-Product Attention**  \n",
    "    For each head:\n",
    "    $$ \\text{Attention}(Q_i, K_i, V_i) = \\text{softmax}\\left(\\frac{Q_iK_i^T}{\\sqrt{d_k}}\\right)V_i $$\n",
    "    where:\n",
    "    - $Q_iK_i^T$ computes similarity scores\n",
    "    - $\\sqrt{d_k}$ scales to prevent vanishing gradients\n",
    "    - softmax normalizes attention weights\n",
    "\n",
    "4. **Multi-head Concatenation**  \n",
    "    Concatenate outputs from all heads:\n",
    "    $$ \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W_O $$\n",
    "    where $W_O \\in \\mathbb{R}^{hd_v \\times d_{model}}$ is the output projection.\n",
    "\n",
    "5. **Final Output**  \n",
    "    $$ \\text{Attention}(X) = \\text{MultiHead}(XW_Q, XW_K, XW_V) $$\n",
    "\n",
    "The complete attention mechanism allows the model to attend to different aspects of the input simultaneously through multiple heads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 14, 768]), torch.Size([1, 14, 768]), torch.Size([1, 14, 768]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_HEADS = 12\n",
    "EMBEDDING_DIM = 768\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value, mask=None, dropout=None):\n",
    "    # Compute the dot product of query and key\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "\n",
    "    # Scale the scores\n",
    "    scores = scores / query.size(-1)**0.5\n",
    "\n",
    "    # Apply the mask (if any)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "    # Apply the softmax function\n",
    "    attention_weights = torch.nn.functional.softmax(scores, dim=-1)\n",
    "\n",
    "    # Apply dropout (if any)\n",
    "    if dropout is not None:\n",
    "        attention_weights = dropout(attention_weights)\n",
    "\n",
    "    # Compute the weighted sum of values\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "attn_weights = model.state_dict()['transformer.h.0.attn.c_attn.weight']\n",
    "attn_biases = model.state_dict()['transformer.h.0.attn.c_attn.bias']\n",
    "\n",
    "# The ln1_output [1, 14, 768] is projected to [1, 14, 768*3] using the attention weights\n",
    "linear_projection = torch.matmul(ln1_output, attn_weights)\n",
    "linear_projection = linear_projection + attn_biases\n",
    "\n",
    "query, key, value = linear_projection.chunk(3, dim=-1)\n",
    "query.shape, key.shape, value.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting to multiple heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 12, 14, 64]),\n",
       " torch.Size([1, 12, 14, 64]),\n",
       " torch.Size([1, 12, 14, 64]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the query, key, and value into multiple heads\n",
    "query = query.view(1, 14, NUM_HEADS, EMBEDDING_DIM // NUM_HEADS).transpose(1, 2)\n",
    "key = key.view(1, 14, NUM_HEADS, EMBEDDING_DIM // NUM_HEADS).transpose(1, 2)\n",
    "value = value.view(1, 14, NUM_HEADS, EMBEDDING_DIM // NUM_HEADS).transpose(1, 2)\n",
    "\n",
    "query.shape, key.shape, value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 12, 14, 64]), torch.Size([1, 12, 14, 14]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply scaled dot-product attention\n",
    "attn_output, attn_weights = scaled_dot_product_attention(query, key, value)\n",
    "attn_output.shape, attn_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 768])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the heads\n",
    "\n",
    "attn_output = attn_output.transpose(1, 2).contiguous().view(1, 14, EMBEDDING_DIM)\n",
    "attn_proj_weight = model.state_dict()['transformer.h.0.attn.c_proj.weight']\n",
    "attn_proj_bias = model.state_dict()['transformer.h.0.attn.c_proj.bias']\n",
    "\n",
    "# Apply the projection layer\n",
    "attn_proj = torch.matmul(attn_output, attn_proj_weight) + attn_proj_bias\n",
    "attn_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 768])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the residual connection\n",
    "attn_output = attn_proj + ln1_output\n",
    "attn_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 768])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply layer normalization\n",
    "ln2_weight = model.state_dict()['transformer.h.0.ln_2.weight']\n",
    "ln2_bias = model.state_dict()['transformer.h.0.ln_2.bias']\n",
    "\n",
    "ln2_output = layer_norm(attn_output, ln2_weight, ln2_bias)\n",
    "ln2_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MultiLayer Perceptron**\n",
    "\n",
    "The MultiLayer Perceptron (MLP) in GPT-2 consists of two linear transformations with a GELU activation in between. The computation flow is:\n",
    "\n",
    "$$ h_{intermediate} = \\text{GELU}(xW_1 + b_1) $$\n",
    "$$ h_{output} = h_{intermediate}W_2 + b_2 $$\n",
    "\n",
    "where:\n",
    "- $x \\in \\mathbb{R}^{L \\times d_{model}}$ is the input\n",
    "- $W_1 \\in \\mathbb{R}^{d_{model} \\times 4d_{model}}$ expands dimensions\n",
    "- $W_2 \\in \\mathbb{R}^{4d_{model} \\times d_{model}}$ projects back\n",
    "- $b_1, b_2$ are bias terms\n",
    "\n",
    "The GELU (Gaussian Error Linear Unit) activation is defined as:\n",
    "\n",
    "$$ \\text{GELU}(x) = x \\cdot \\Phi(x) $$\n",
    "\n",
    "where $\\Phi(x)$ is the cumulative distribution function of the standard normal distribution:\n",
    "\n",
    "$$ \\Phi(x) = \\frac{1}{2}\\left[1 + \\text{erf}\\left(\\frac{x}{\\sqrt{2}}\\right)\\right] $$\n",
    "\n",
    "GELU can be approximated as:\n",
    "\n",
    "$$ \\text{GELU}(x) \\approx 0.5x\\left(1 + \\tanh\\left[\\sqrt{2/\\pi}(x + 0.044715x^3)\\right]\\right) $$\n",
    "\n",
    "In GPT-2:\n",
    "- Input dimension ($d_{model}$): 768\n",
    "- Intermediate dimension ($4d_{model}$): 3072\n",
    "- Output dimension ($d_{model}$): 768\n",
    "\n",
    "The complete MLP computation in a transformer block:\n",
    "\n",
    "$$ \\text{MLP}(x) = \\text{LayerNorm}(x + \\text{FFN}(x)) $$\n",
    "\n",
    "where FFN is the feed-forward network:\n",
    "\n",
    "$$ \\text{FFN}(x) = \\text{GELU}(xW_1 + b_1)W_2 + b_2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([768, 3072]),\n",
       " torch.Size([3072]),\n",
       " torch.Size([3072, 768]),\n",
       " torch.Size([768]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_weight1 = model.state_dict()['transformer.h.0.mlp.c_fc.weight']\n",
    "mlp_bias1 = model.state_dict()['transformer.h.0.mlp.c_fc.bias']\n",
    "\n",
    "mlp_weight2 = model.state_dict()['transformer.h.0.mlp.c_proj.weight']\n",
    "mlp_bias2 = model.state_dict()['transformer.h.0.mlp.c_proj.bias']\n",
    "\n",
    "mlp_weight1.shape, mlp_bias1.shape, mlp_weight2.shape, mlp_bias2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 768])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the first linear layer\n",
    "mlp_output = torch.matmul(ln2_output, mlp_weight1) + mlp_bias1\n",
    "# Apply the GELU activation function\n",
    "mlp_output = torch.nn.functional.gelu(mlp_output)\n",
    "# Apply the second linear layer\n",
    "mlp_output = torch.matmul(mlp_output, mlp_weight2) + mlp_bias2\n",
    "mlp_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 768])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_output = mlp_output + ln2_output\n",
    "block_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 768])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight torch.Size([50257, 768])\n",
      "transformer.wpe.weight torch.Size([1024, 768])\n",
      "transformer.h.0.ln_1.weight torch.Size([768])\n",
      "transformer.h.0.ln_1.bias torch.Size([768])\n",
      "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.0.ln_2.weight torch.Size([768])\n",
      "transformer.h.0.ln_2.bias torch.Size([768])\n",
      "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_1.weight torch.Size([768])\n",
      "transformer.h.1.ln_1.bias torch.Size([768])\n",
      "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_2.weight torch.Size([768])\n",
      "transformer.h.1.ln_2.bias torch.Size([768])\n",
      "transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_1.weight torch.Size([768])\n",
      "transformer.h.2.ln_1.bias torch.Size([768])\n",
      "transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.2.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.2.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.2.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_2.weight torch.Size([768])\n",
      "transformer.h.2.ln_2.bias torch.Size([768])\n",
      "transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.2.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.2.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_1.weight torch.Size([768])\n",
      "transformer.h.3.ln_1.bias torch.Size([768])\n",
      "transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.3.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.3.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.3.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_2.weight torch.Size([768])\n",
      "transformer.h.3.ln_2.bias torch.Size([768])\n",
      "transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.3.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.3.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_1.weight torch.Size([768])\n",
      "transformer.h.4.ln_1.bias torch.Size([768])\n",
      "transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.4.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.4.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.4.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_2.weight torch.Size([768])\n",
      "transformer.h.4.ln_2.bias torch.Size([768])\n",
      "transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.4.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.4.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_1.weight torch.Size([768])\n",
      "transformer.h.5.ln_1.bias torch.Size([768])\n",
      "transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.5.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.5.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.5.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_2.weight torch.Size([768])\n",
      "transformer.h.5.ln_2.bias torch.Size([768])\n",
      "transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.5.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_1.weight torch.Size([768])\n",
      "transformer.h.6.ln_1.bias torch.Size([768])\n",
      "transformer.h.6.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.6.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.6.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.6.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_2.weight torch.Size([768])\n",
      "transformer.h.6.ln_2.bias torch.Size([768])\n",
      "transformer.h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.6.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.6.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_1.weight torch.Size([768])\n",
      "transformer.h.7.ln_1.bias torch.Size([768])\n",
      "transformer.h.7.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.7.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.7.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.7.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_2.weight torch.Size([768])\n",
      "transformer.h.7.ln_2.bias torch.Size([768])\n",
      "transformer.h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.7.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.7.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_1.weight torch.Size([768])\n",
      "transformer.h.8.ln_1.bias torch.Size([768])\n",
      "transformer.h.8.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.8.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.8.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.8.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_2.weight torch.Size([768])\n",
      "transformer.h.8.ln_2.bias torch.Size([768])\n",
      "transformer.h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.8.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.8.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_1.weight torch.Size([768])\n",
      "transformer.h.9.ln_1.bias torch.Size([768])\n",
      "transformer.h.9.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.9.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.9.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.9.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_2.weight torch.Size([768])\n",
      "transformer.h.9.ln_2.bias torch.Size([768])\n",
      "transformer.h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.9.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.9.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_1.weight torch.Size([768])\n",
      "transformer.h.10.ln_1.bias torch.Size([768])\n",
      "transformer.h.10.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.10.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.10.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.10.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_2.weight torch.Size([768])\n",
      "transformer.h.10.ln_2.bias torch.Size([768])\n",
      "transformer.h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.10.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.10.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_1.weight torch.Size([768])\n",
      "transformer.h.11.ln_1.bias torch.Size([768])\n",
      "transformer.h.11.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.11.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.11.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.11.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_2.weight torch.Size([768])\n",
      "transformer.h.11.ln_2.bias torch.Size([768])\n",
      "transformer.h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.11.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.ln_f.weight torch.Size([768])\n",
      "transformer.ln_f.bias torch.Size([768])\n",
      "lm_head.weight torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "# Extracting parameters and saving them as a file\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\") # 124M parameters\n",
    "\n",
    "output_dir = 'parameters/gpt2'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for k, v in model.state_dict().items():\n",
    "    print(k, v.shape)\n",
    "    np.save(os.path.join(output_dir, k), v.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
